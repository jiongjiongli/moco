{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moco (momentum contrast) - simplified cpu implementation\n",
    "\n",
    "this notebook implements a simplified version of moco for self-supervised learning on cifar-10.\n",
    "\n",
    "**paper**: [momentum contrast for unsupervised visual representation learning (cvpr 2020)](https://arxiv.org/abs/1911.05722)\n",
    "\n",
    "**original code**: https://github.com/facebookresearch/moco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set device\n",
    "device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. data augmentation\n",
    "\n",
    "moco uses two different augmented views of the same image as positive pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoCropsTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        q = self.base_transform(x)\n",
    "        k = self.base_transform(x)\n",
    "        return [q, k]\n",
    "\n",
    "# augmentation for cifar-10 (32x32 images)\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# test set transform (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. load cifar-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 170M/170M [00:22<00:00, 7.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# training dataset with two augmentations\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=TwoCropsTransform(augmentation)\n",
    ")\n",
    "\n",
    "# test dataset for evaluation\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. moco model implementation\n",
    "\n",
    "key components:\n",
    "- **query encoder**: trainable encoder\n",
    "- **momentum encoder**: slowly updated copy of query encoder\n",
    "- **queue**: stores previous key embeddings as negative samples\n",
    "- **contrastive loss**: infonce loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoCo(nn.Module):\n",
    "    def __init__(self, base_encoder, dim=128, K=1024, m=0.999, T=0.07):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_encoder: backbone network (e.g., ResNet-18)\n",
    "            dim: feature dimension (default: 128)\n",
    "            K: queue size (default: 1024)\n",
    "            m: momentum for updating key encoder (default: 0.999)\n",
    "            T: temperature parameter (default: 0.07)\n",
    "        \"\"\"\n",
    "        super(MoCo, self).__init__()\n",
    "        \n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        \n",
    "        # create query encoder\n",
    "        self.encoder_q = base_encoder(num_classes=dim)\n",
    "        \n",
    "        # create momentum encoder\n",
    "        self.encoder_k = base_encoder(num_classes=dim)\n",
    "        \n",
    "        # initialize momentum encoder with query encoder weights\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)\n",
    "            param_k.requires_grad = False  # no gradient for momentum encoder\n",
    "        \n",
    "        # create the queue\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = F.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"Momentum update of the key encoder\"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1.0 - self.m)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        \"\"\"Update queue: enqueue current keys, dequeue oldest\"\"\"\n",
    "        batch_size = keys.shape[0]\n",
    "        \n",
    "        ptr = int(self.queue_ptr)\n",
    "        \n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        if ptr + batch_size <= self.K:\n",
    "            self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        else:\n",
    "            # wrap around\n",
    "            remaining = self.K - ptr\n",
    "            self.queue[:, ptr:] = keys[:remaining].T\n",
    "            self.queue[:, :batch_size - remaining] = keys[remaining:].T\n",
    "        \n",
    "        ptr = (ptr + batch_size) % self.K  # move pointer\n",
    "        self.queue_ptr[0] = ptr\n",
    "    \n",
    "    def forward(self, im_q, im_k):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            im_q: a batch of query images\n",
    "            im_k: a batch of key images\n",
    "        Output:\n",
    "            logits, targets\n",
    "        \"\"\"\n",
    "        # compute query features\n",
    "        q = self.encoder_q(im_q)  # nxc\n",
    "        q = F.normalize(q, dim=1)\n",
    "        \n",
    "        # compute key features (no gradient)\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update_key_encoder()\n",
    "            k = self.encoder_k(im_k)  # nxc\n",
    "            k = F.normalize(k, dim=1)\n",
    "        \n",
    "        # compute logits\n",
    "        # positive logits: nx1\n",
    "        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n",
    "        # negative logits: nxk\n",
    "        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n",
    "        \n",
    "        # logits: nx(1+k)\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
    "        \n",
    "        # apply temperature\n",
    "        logits /= self.T\n",
    "        \n",
    "        # labels: positives are the 0-th\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=logits.device)\n",
    "        \n",
    "        # dequeue and enqueue\n",
    "        self._dequeue_and_enqueue(k)\n",
    "        \n",
    "        return logits, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. create resnet-18 encoder for cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18_cifar(num_classes=128):\n",
    "    \"\"\"ResNet-18 adapted for CIFAR-10 (32x32 images)\"\"\"\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    \n",
    "    # modify first conv layer for 32x32 images\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()  # remove maxpool for small images\n",
    "    \n",
    "    # modify final layer\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rifatxia/Desktop/TensorstoreWork/tensorstore/llama-work/llama-venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rifatxia/Desktop/TensorstoreWork/tensorstore/llama-work/llama-venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 11,234,496\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "dim = 128  # feature dimension\n",
    "K = 1024  # queue size\n",
    "m = 0.999  # momentum\n",
    "T = 0.07  # temperature\n",
    "lr = 0.03  # learning rate\n",
    "epochs = 10  # number of epochs\n",
    "\n",
    "# create model\n",
    "model = MoCo(\n",
    "    base_encoder=get_resnet18_cifar,\n",
    "    dim=dim,\n",
    "    K=K,\n",
    "    m=m,\n",
    "    T=T\n",
    ").to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MoCo pre-training...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 1563/1563 [23:05<00:00,  1.13it/s, loss=4.8265, acc=13.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.1421, Acc: 13.21%\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|██▋           | 299/1563 [04:38<20:24,  1.03it/s, loss=4.0800, acc=18.29%]"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, _ in pbar:\n",
    "        # images is a list [im_q, im_k]\n",
    "        im_q = images[0].to(device)\n",
    "        im_k = images[1].to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        logits, labels = model(im_q, im_k)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# training\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "print('Starting MoCo pre-training...')\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "    loss, acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(loss)\n",
    "    train_accs.append(acc)\n",
    "    print(f'Loss: {loss:.4f}, Acc: {acc:.2f}%')\n",
    "\n",
    "print('\\nPre-training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. plot training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(train_accs)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training Accuracy (Contrastive)')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. linear evaluation\n",
    "\n",
    "evaluate the learned representations by training a linear classifier on top of the frozen encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=10):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # freeze encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(x)\n",
    "        return self.fc(features)\n",
    "\n",
    "# create classifier\n",
    "classifier = LinearClassifier(model.encoder_q).to(device)\n",
    "\n",
    "# new optimizer for linear layer only\n",
    "optimizer_linear = optim.SGD(classifier.fc.parameters(), lr=30.0, momentum=0.9, weight_decay=0)\n",
    "criterion_linear = nn.CrossEntropyLoss()\n",
    "\n",
    "# load training data without augmentation for linear eval\n",
    "train_dataset_linear = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=test_transform\n",
    ")\n",
    "train_loader_linear = DataLoader(train_dataset_linear, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Training Linear'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def test_linear(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# train linear classifier\n",
    "print('\\nLinear Evaluation...')\n",
    "linear_epochs = 20\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(linear_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{linear_epochs}')\n",
    "    train_loss, train_acc = train_linear(classifier, train_loader_linear, optimizer_linear, criterion_linear, device)\n",
    "    test_loss, test_acc = test_linear(classifier, test_loader, criterion_linear, device)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "\n",
    "print(f'\\nBest Test Accuracy: {best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pre-trained encoder\n",
    "torch.save({\n",
    "    'encoder': model.encoder_q.state_dict(),\n",
    "    'classifier': classifier.fc.state_dict(),\n",
    "    'best_acc': best_acc,\n",
    "}, 'moco_cifar10.pth')\n",
    "\n",
    "print('Model saved to moco_cifar10.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "\n",
    "this notebook demonstrates a simplified cpu-friendly implementation of moco:\n",
    "\n",
    "1. **pre-training**: learned visual representations using contrastive learning\n",
    "2. **linear evaluation**: evaluated learned features by training a linear classifier\n",
    "\n",
    "### key simplifications:\n",
    "- small backbone (resnet-18)\n",
    "- small queue (k=1024)\n",
    "- small dataset (cifar-10)\n",
    "- cpu training\n",
    "- fewer epochs\n",
    "\n",
    "### expected results:\n",
    "- with proper training (more epochs), moco can achieve 70-80% accuracy on cifar-10\n",
    "- this simplified version should achieve 50-60% accuracy\n",
    "\n",
    "### next steps:\n",
    "1. train for more epochs (100+)\n",
    "2. increase queue size (k=4096)\n",
    "3. use gpu for faster training\n",
    "4. try moco v2 improvements (stronger augmentation, mlp projection head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Llama Venv",
   "language": "python",
   "name": "llama-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
